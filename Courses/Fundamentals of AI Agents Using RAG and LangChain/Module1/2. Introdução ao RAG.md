---
Course: Fundamentals of AI Agents Using RAG and LangChain
Module: Module 2
---
## Processo RAG 
Retrieval Augmented Generation (*RAG*) é um framework de IA que otimiza a saída de um modelo LLM sem necessitar retreinar o modelo.

Modelos LLM são bons em tarefas genéricas, mas em conjuntos de dados que não fizeram parte do treinamento o desempenho pode ser baixo. Para melhorar a precisão de respostas sobre esses conjuntos de dados são adicionadas novas fontes de dados.

## Etapas do processo RAG
Tanto o prompt do usuário quanto os dados que formam a base de consulta precisam estar codificados em um formato vetorial. Esse processo é feito pelos *encoders*.

Por meio da distância dos vetores prompt e base de dados são criadas métricas a fim de escolher os **k** vetores que serão retornados.

Uma nova query é criada passando os dados de contexto e o prompt original do usuário. Esse é o processo de criação da *consulta aumentada*.

Outro modelo LLM processa a query e gera uma resposta para o prompt do usuário. Por receber o contexto diretamente na query o grau de alucinação é muito menor.

A resposta pode passar por um processo de pós-processamento para ser refinada antes de devolvida para o usuário.


### Prompt Encoding

#### 1. Tokenização do prompt
O prompt é convertido em tokens, que podem ser palavras ou sub-palavras. Esse processo é feito por um modelo de embeddings pré-treinado como o *BERT* ou *GPT*.

#### 2. Conversão de tokens em vetores
Usando um modelo de embedding os tokens são convertidos para vetores de alta dimensionalidade.

#### 3. Calculo do vetor representativo
Com base nos vetores criados a partir dos tokens é calculado um novo vetor resultado da média dos vetores de token. Esse vetor é a representação vetorial do prompt.


### Knowledge Base Encoding

#### 1. Divisão dos dados em chunks
Para ganhar eficiência durante a recuperação os dados são divididos em chunks. Chunks podem ter o tamanho de páginas de um PDF, parágrafos ou um numero ainda menor de caracteres.

#### 2. Tokenização dos chunks
Assim como no processo de tokenização do prompt cada chunk é convertido em tokens por modelos de embeddings.

#### 3. Conversão de tokens em vetores
Os tokens são convertidos em vetores de alta dimensionalidade através de modelos de embeddings.

#### 4. Criação do vetor representativo
Um vetor representativo é criado calculando a média de todos os vetores gerados pelo modelo de embeddings.

#### 5. Armazenamento em um banco de Dados Vetorial
Os vetores gerados para cada chunk são armazenados em um *banco de dados vetorial*. Cada vetor é indexado com um id e metadados relevantes.

Exemplos de banco de dados vetoriais são:
- FAISS
- Pinecone
- Weaviante
- ChromaDB

### Mecanismo de busca de contexto
O vetor prompt é comparado com os vetores da base de conhecimento usando métricas de similaridade como *produto escalar* e *distância cosseno*.

O *produto escalar* é uma métrica que mede o alinhamento entre vetores, levando em consideração direção e magnitude.

A *distância cosseno* mede o ângulo entre vetores, priorizando a direção.

Os k chunks mais próximos do vetor prompt são selecionados para recuperação. O numero k é um *hiperparâmetro* que pode ser ajustado para uma melhor recuperação de dados.

### Criação da consulta aumentada
Um novo prompt é gerado concatenando os chunks selecionados e o prompt original. O novo prompt é então processado por um LLM que gera uma resposta levando em consideração o contexto fornecido.
